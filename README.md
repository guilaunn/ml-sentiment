# Classificador de Sentimentos com BERT + FastAPI

Projeto desenvolvido para desafio prÃ¡tico de **Arquiteto de Machine Learning SÃªnior**, com foco no ciclo completo de um sistema de Machine Learning â€” desde o fine-tuning atÃ© a disponibilizaÃ§Ã£o via API RESTful.

---

## ğŸ“ Estrutura do Projeto

```
ml-sentiment/
â”œâ”€â”€ training/
â”‚   â””â”€â”€ fine_tune.py          # Script de fine-tuning com BERT
â”œâ”€â”€ api/
â”‚   â””â”€â”€ main.py               # API RESTful com FastAPI
â”œâ”€â”€ models/
â”‚   â””â”€â”€ sentiment_model/      # Artefatos salvos (modelo + tokenizer)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
```

---

## ğŸ“Œ Dataset

Utilizei o dataset pÃºblico [`b2w-reviews01`](https://github.com/americanas-tech/b2w-reviews01), contendo mais de 15 mil avaliaÃ§Ãµes de produtos. O campo `overall_rating` foi mapeado em:

- 1 ou 2 â†’ **Negativo** (`0`)
- 4 ou 5 â†’ **Positivo** (`1`)
- 3 â†’ Removido por ser neutro

---

## ğŸ§ª Fine-tuning com BERT

Modelo base: [`neuralmind/bert-base-portuguese-cased`](https://huggingface.co/neuralmind/bert-base-portuguese-cased)

### ğŸ“¥ Executar fine-tuning

```bash
python training/fine_tune.py
```

O modelo treinado serÃ¡ salvo em:

```
./models/sentiment_model/
```

---

## ğŸš€ API RESTful com FastAPI

ApÃ³s o treinamento, disponibilizamos o modelo via uma API.

### ğŸ“ Arquivo: `api/main.py`

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import BertTokenizer, BertForSequenceClassification
import torch

app = FastAPI()

MODEL_PATH = "./models/sentiment_model"
tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
model = BertForSequenceClassification.from_pretrained(MODEL_PATH)

class TextInput(BaseModel):
    text: str

@app.post("/predict")
def predict(input_data: TextInput):
    if not input_data.text.strip():
        raise HTTPException(status_code=400, detail="Texto nÃ£o pode estar vazio.")

    inputs = tokenizer(
        input_data.text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=128,
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.nn.functional.softmax(outputs.logits, dim=1)
        pred_class = torch.argmax(probs).item()
        score = round(probs[0][pred_class].item(), 4)

    label = "positivo" if pred_class == 1 else "negativo"
    return {"sentiment": label, "score": score}
```

---

### â–¶ï¸ Rodar a API localmente

```bash
uvicorn api.main:app --reload
```

Acesse em: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

---

### ğŸ§ª Exemplo de chamada com `curl`

```bash
curl -X POST http://localhost:8000/predict   -H "Content-Type: application/json"   -d '{"text": "Produto excelente, chegou antes do prazo!"}'
```

**Resposta:**
```json
{
  "sentiment": "positivo",
  "score": 0.9821
}
```

---

## ğŸ’¡ DecisÃµes TÃ©cnicas

- **Modelo**: `bert-base-portuguese-cased` foi escolhido por ser um modelo BERT treinado com corpus em portuguÃªs, garantindo melhor compreensÃ£o dos textos da base.
- **PrÃ©-processamento**: Filtrei textos vazios/nulos e removi avaliaÃ§Ãµes neutras (nota 3), para melhor desempenho binÃ¡rio.
- **Treinamento**: Realizado com `Trainer` da Hugging Face e mÃ©tricas de `accuracy` e `f1`.
- **API**: Implementada com FastAPI por sua performance e documentaÃ§Ã£o automÃ¡tica.

---

## ğŸ“ˆ PossÃ­vel EvoluÃ§Ã£o para ProduÃ§Ã£o

- Servir com **TorchServe** ou **Triton** para maior escalabilidade.
- ContainerizaÃ§Ã£o com **Docker**.
- Monitoramento com Prometheus + Grafana.
- Logging estruturado.
- Versionamento de modelo com MLflow.
- Load balancing com Nginx ou Kubernetes.

---

## ğŸ“¦ Requisitos

Crie um ambiente virtual e instale as dependÃªncias:

```bash
pip install -r requirements.txt
```
